ğŸ§  Network Flow Data ETL Pipeline (PostgreSQL + Pandas + Python)
ğŸ“‹ Overview
This project demonstrates a scalable ETL (Extract, Transform, Load) pipeline for network flow data using Python, Pandas, and PostgreSQL. It processes network intrusion detection datasets containing network flow features for cybersecurity analysis.

âš™ï¸ Features
âœ… Automated Database Connection - Connects to PostgreSQL using configuration
âœ… Dynamic Table Creation - Creates table with proper schema based on network flow features
âœ… Efficient Data Loading - Reads large CSV files using pandas chunks to handle memory constraints
âœ… Data Quality Checks - Identifies and handles missing values and duplicates
âœ… Batch Processing - Loads data efficiently using psycopg2.extras.execute_batch()
âœ… Automatic Schema Management - Creates table with appropriate data types for network flow data

ğŸ“Š Dataset Information
The pipeline processes network intrusion detection datasets from the Canadian Institute for Cybersecurity:

Primary Dataset Sources:
. CIC-IDS2017 Cleaned and Preprocessed - Preprocessed version of the CIC-IDS2017 dataset

. Network Intrusion Dataset - Additional network intrusion data from CIC

Dataset Contents:
Network flow statistics (duration, packet counts, packet lengths)

Protocol features (TCP flags, window sizes)

Timing statistics (inter-arrival times, active/idle times)

Attack classification labels for intrusion detection

Various attack types including DDoS, Brute Force, Infiltration, Web Attacks, and more

ğŸ”„ ETL Process Steps
1. Extract
Reads network flow CSV files in chunks to handle large files

Combines chunks into a single DataFrame for processing

Supports multiple CIC datasets for comprehensive analysis

2. Transform
Missing Value Treatment:

Numeric columns filled with mean values

Categorical columns filled with mode values

Duplicate Removal: Identifies and removes duplicate rows across all features

Data Validation: Checks data quality and structure

Type Conversion: Ensures proper data types for network features

3. Load
Creates optimized PostgreSQL table with proper data types

Uses batch insertion for high-performance loading

Commits data in manageable chunks (100,000 records)

Maintains data integrity through transaction management

ğŸ—„ï¸ Database Schema
The pipeline creates a table with the following structure:

flow_id (SERIAL PRIMARY KEY) - Unique identifier

Network features: destination_port, flow_duration, total_fwd_packets, etc.

Statistical features: flow_iat_mean, flow_iat_std, active_mean, etc.

Protocol flags: fin_flag_count, rst_flag_count, ack_flag_count, etc.

Classification: attack_type - Target variable for intrusion detection

ğŸ“ Project Structure
text
Big_data/
â”‚
â”œâ”€â”€ Big_data.py              # Main ETL script
â”œâ”€â”€ db_config.py             # Database connection configuration
â”œâ”€â”€ cic_ids_2017.csv         # Input dataset (network flow data)
â””â”€â”€ README.md                # Project documentation
ğŸš€ Usage
Configure Database: Update db_config.py with your PostgreSQL credentials

Prepare Data: Download datasets from Kaggle and ensure CSV files are in the project directory

Run Pipeline: Execute the script and enter the desired table name when prompted

Monitor Progress: The script provides real-time updates on data processing stages

âš¡ Performance Optimizations
Chunked Reading: Handles large files without memory overflow

Batch Insertion: Uses execute_batch for efficient database operations

Transaction Management: Commits in chunks to maintain database performance

Data Type Optimization: Uses appropriate PostgreSQL data types for each feature

ğŸ¯ Use Cases
Network Security Analysis - Process and store network flow data for intrusion detection

Machine Learning Pipelines - Prepare cleaned datasets for security ML models

Security Research - Efficiently manage and query large network datasets

Data Warehousing - Build structured repositories for network security analytics

Threat Intelligence - Analyze patterns across multiple cybersecurity datasets

ğŸ”¬ About Canadian Institute for Cybersecurity
The datasets used in this project are provided by the Canadian Institute for Cybersecurity (CIC), a leading research institution focused on cybersecurity. CIC produces realistic benchmark datasets for network intrusion detection systems that are widely used in academic and industrial research.

ğŸ“ˆ Output
Clean, deduplicated network flow data in PostgreSQL

Comprehensive data quality report

Structured database ready for analysis and modeling

Scalable pipeline capable of handling large-scale network datasets

Prepared data for machine learning and security analytics

